# Sarcasm-detection

This is my attempt at making a sentiment-classifier which is slightly different from the conventional positive-negative sentiment
classifier. I attempt to differentiate between 'sarcasm' and 'facts'.

## Order of files to run project on your PC

1. twitter-access.py - creates a text file containing tweets
2. preprocessing.py - creates a text file containing processed tweets
3. detection.py - does the actual classification

**1. Pulling tweets from Twitter**

Corresponding file : twitter-access.py

I have used tweets pulled from twitter using the twitter API. If you are unaware of how to pull tweets from twitter, here are a few
YouTube tutorials to help you.

1. [Sentdex - Streaming via Twitter API](https://www.youtube.com/watch?v=pUUxmvvl2FE)
2. [Introduction to the Twitter API](https://www.youtube.com/watch?v=wlnx-7cm4Gg)

I have also added my code in this repository for your reference.

**__PS__** : You cannot run the code as it is. This is because the consumer key, consumer secret, access key and access secret variables are
unique to my Twitter account (The values in the code are dummy names and will not grant you access to my account). You can follow the links
shared above and replace the dummy values with values generated by the API for your account.

**2. Pre-processing tweets**

Corresponding file : preprocessing.py

This is the most important step, arguably more important than the classification itself. This is also very labour-intensive.

Once we obtain tweets corresponding to #facts and #sarcasm, we remove any punctuations, links, hashtags and tags to other twitter accounts.
We accomplish this using [regular expressions](https://en.wikipedia.org/wiki/Regular_expression). If you aren't familiar with using regular
expressions, the following tutorials will bring you up to speed.

1. [Introduction to Regular Expressions](https://www.youtube.com/watch?v=sa-TUpSx1JA)
2. [Using the re library in Python](https://www.youtube.com/watch?v=K8L6KVGG-7o)

**3. Sentiment Classification**

In order to perform sentiment classification, I have used the nltk library as opposed to sklearn. This is because the freely avaiable
[book](https://www.nltk.org/book/) provided by the creators of nltk was my first introduction to Machine Learning / Natural Language Processing. You can try using sklearn as well, which is more user-friendly and generic.

While defining the classifier, one can choose features to classify upon. In my experience, using unigrams, bigrams and trigrams are the best option. They tend to classify labelled text based on the frequency of occurence of certain words/phrases. I used the Naive Bayes classifier here, though other classifiers would work as well.

If you want to learn more about sentiment classification using nltk, the following YouTube playlist will help you.

1. [Sentdex nltk tutorial](https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL)

**Takeaways from this project**
1. I learned to access data from the internet and to represent unstructured data in a structured and easy-to-access manner.
2. I learned to use regular expressions.
3. I learned about basic classification algorithms and their implementations using nltk.

Based on the output of the project, one can see that basic classification algorithms work fine. There is a lot of scope for improvement and deep learning methods using lstms and grus are much more suited for understanding sentiments / semantic relations among words of a sentence.

**Coming soon** : Additional features to improve accuracy of sentiment classification.

# References 

1. [Miruna Pislar Github](https://github.com/MirunaPislar/Sarcasm-Detection)
2. [Sarcasm Detector website](http://www.thesarcasmdetector.com/)






